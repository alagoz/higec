# HiGEC: Hierarchy Generation and Extended Classification Framework
HiGEC is a Python framework for performing hierarchical classification with automated hierarchy generation, flexible exploitation strategies, and integration with modern classifiers.

HiGeC is evaluated on 100 multi-class datasets, demonstrating significant improvements over traditional flat classification (FC) approaches, particularly with advanced classifiers like XGBoost, RF, ETC, and LGBM.

![fig_cdd_hge_vs_FC_vs_OVA_f1]()

| name                     | RF           | XGBoost          | ETC          | LGBM          | LCN[XGB]+      | LCPN[ETC]+F[XGB] | LCPN[RF]+F[XGB] | LCPN[XGB]+F[XGB] |
|--------------------------|--------------|------------------|--------------|---------------|----------------|------------------|-----------------|------------------|
| air-quality-and-pollution| 0.9291Â±0.0089 | 0.9284Â±0.0116 | 0.9247Â±0.0110 | 0.9290Â±0.0105 | 0.9271Â±0.0119 | 0.9291Â±0.0114 | 0.9289Â±0.0112 | 0.9290Â±0.0101 |
| alizadeh-2000-v2         | 0.9485Â±0.0630 | 0.8594Â±0.1081 | 0.9743Â±0.0515 | 0.9440Â±0.0668 | 0.8841Â±0.1039 | 0.8809Â±0.1219 | 0.8681Â±0.1153 | 0.9357Â±0.0643 |
| amazon-commerce-reviews   | 0.6670Â±0.0281 | 0.7090Â±0.0253 | 0.7273Â±0.0098 | 0.7077Â±0.0294 | 0.5945Â±0.0203 | 0.7402Â±0.0221 | 0.7257Â±0.0262 | 0.5890Â±0.0157 |
| analcatdata-halloffame    | 0.6588Â±0.0652 | 0.6516Â±0.0563 | 0.6663Â±0.0507 | 0.6490Â±0.0648 | 0.6565Â±0.0733 | 0.6530Â±0.0621 | 0.6532Â±0.0624 | 0.6474Â±0.0738 |
| analcatdata-happiness     | 0.3204Â±0.1043 | 0.3990Â±0.0990 | 0.2605Â±0.1078 | 0.5259Â±0.1395 | 0.5285Â±0.0643 | 0.4019Â±0.1092 | 0.4019Â±0.1092 | 0.3601Â±0.0962 |
| anneal                    | 0.7733Â±0.0739 | 0.7976Â±0.0746 | 0.7814Â±0.0627 | 0.7722Â±0.0749 | 0.7690Â±0.0866 | 0.7995Â±0.0811 | 0.8021Â±0.0887 | 0.7871Â±0.0717 |
| arrythmia                 | 0.5761Â±0.0438 | 0.5924Â±0.0744 | 0.5257Â±0.0488 | 0.5946Â±0.0609 | 0.6397Â±0.0380 | 0.6305Â±0.0654 | 0.6281Â±0.0664 | 0.6267Â±0.0755 |
| artificial-characters     | 0.9102Â±0.0058 | 0.8998Â±0.0078 | 0.9095Â±0.0065 | 0.9119Â±0.0081 | 0.8917Â±0.0055 | 0.9199Â±0.0060 | 0.9139Â±0.0067 | 0.9102Â±0.0069 |
| auto-ml-selector          | 0.3392Â±0.0961 | 0.3710Â±0.1210 | 0.2765Â±0.0984 | 0.3560Â±0.1240 | 0.3635Â±0.1234 | 0.3682Â±0.1178 | 0.3782Â±0.1265 | 0.3922Â±0.1208 |
| auto-univ-au4-2500        | 0.4071Â±0.0133 | 0.4562Â±0.0118 | 0.3850Â±0.0127 | 0.4627Â±0.0145 | 0.4670Â±0.0118 | 0.4560Â±0.0119 | 0.4560Â±0.0119 | 0.4071Â±0.0133 |
| auto-univ-au7-1100        | 0.3775Â±0.0307 | 0.3680Â±0.0245 | 0.3614Â±0.0357 | 0.3701Â±0.0338 | 0.3813Â±0.0320 | 0.3785Â±0.0342 | 0.3772Â±0.0261 | 0.3598Â±0.0346 |
| bach                      | 0.5470Â±0.0379 | 0.5756Â±0.0389 | 0.5575Â±0.0381 | 0.0231Â±0.0127 | 0.5916Â±0.0295 | 0.5876Â±0.0374 | 0.5823Â±0.0395 | 0.5447Â±0.0353 |
| baseball                  | 0.6634Â±0.0590 | 0.6516Â±0.0563 | 0.6663Â±0.0507 | 0.6490Â±0.0648 | 0.6565Â±0.0733 | 0.6530Â±0.0621 | 0.6532Â±0.0624 | 0.6439Â±0.0713 |
| bridges                   | 0.5828Â±0.1390 | 0.5335Â±0.0787 | 0.5567Â±0.0822 | 0.4521Â±0.1287 | 0.5701Â±0.0998 | 0.5448Â±0.0780 | 0.5582Â±0.1020 | 0.5771Â±0.1267 |
| calendar-dow              | 0.5704Â±0.0303 | 0.5569Â±0.0385 | 0.5794Â±0.0415 | 0.5620Â±0.0320 | 0.5655Â±0.0320 | 0.5669Â±0.0278 | 0.5651Â±0.0304 | 0.5730Â±0.0283 |
| cars                      | 0.7804Â±0.0451 | 0.7802Â±0.0356 | 0.7710Â±0.0384 | 0.7790Â±0.0671 | 0.7782Â±0.0398 | 0.7797Â±0.0415 | 0.7824Â±0.0436 | 0.7863Â±0.0533 |
| cervical-cancer-risk      | 0.2480Â±0.0224 | 0.2315Â±0.0283 | 0.2510Â±0.0198 | 0.2082Â±0.0202 | 0.2234Â±0.0243 | 0.2328Â±0.0281 | 0.2304Â±0.0287 | 0.2454Â±0.0207 |
| cleveland-nominal         | 0.2638Â±0.0286 | 0.2663Â±0.0453 | 0.2825Â±0.0412 | 0.2605Â±0.0307 | 0.2519Â±0.0434 | 0.2669Â±0.0565 | 0.2604Â±0.0490 | 0.2601Â±0.0439 |
| cnae-9                    | 0.9304Â±0.0055 | 0.9194Â±0.0154 | 0.9405Â±0.0139 | 0.8423Â±0.0145 | 0.8958Â±0.0143 | 0.9255Â±0.0152 | 0.9242Â±0.0165 | 0.9291Â±0.0104 |
| collins                   | 0.2055Â±0.0232 | 0.2030Â±0.0180 | 0.2057Â±0.0199 | 0.1932Â±0.0213 | 0.1868Â±0.0181 | 0.2164Â±0.0183 | 0.2088Â±0.0141 | 0.2033Â±0.0224 |
| deng-reads                | 0.9339Â±0.0218 | 0.9430Â±0.0197 | 0.9368Â±0.0126 | 0.9540Â±0.0563 | 0.9750Â±0.0244 | 0.9623Â±0.0154 | 0.9623Â±0.0154 | 0.9382Â±0.0211 |
| dgf                       | 0.6104Â±0.0533 | 0.6355Â±0.0690 | 0.6394Â±0.0533 | 0.6333Â±0.0725 | 0.6352Â±0.0960 | 0.6506Â±0.0920 | 0.6347Â±0.0676 | 0.6151Â±0.0719 |
| diabetes-130-us           | 0.3920Â±0.0024 | 0.4066Â±0.0031 | 0.3906Â±0.0023 | 0.3999Â±0.0033 | 0.3563Â±0.0028 | 0.3481Â±0.0023 | 0.3505Â±0.0022 | 0.3426Â±0.0027 |
| diggle-table-a2           | 0.9585Â±0.0304 | 0.9628Â±0.0144 | 0.9747Â±0.0186 | 0.9791Â±0.0113 | 0.9690Â±0.0175 | 0.9644Â±0.0143 | 0.9643Â±0.0143 | 0.9731Â±0.0168 |
| eda-mortgage-ny           | 0.5001Â±0.0036 | 0.4850Â±0.0031 | 0.4927Â±0.0031 | 0.4824Â±0.0038 | 0.4681Â±0.0051 | 0.4861Â±0.0036 | 0.4823Â±0.0022 | 0.4845Â±0.0042 |
| energy-efficiency         | 0.6951Â±0.0253 | 0.6972Â±0.0373 | 0.6870Â±0.0363 | 0.6857Â±0.0261 | 0.6951Â±0.0388 | 0.7036Â±0.0372 | 0.6946Â±0.0417 | 0.6948Â±0.0309 |
| financial-risk-assessment | 0.2796Â±0.0039 | 0.3045Â±0.0065 | 0.2999Â±0.0117 | 0.2686Â±0.0032 | 0.3098Â±0.0115 | 0.3081Â±0.0184 | 0.3076Â±0.0192 | 0.2942Â±0.0216 |
| flags                     | 0.5562Â±0.0746 | 0.5597Â±0.0591 | 0.5161Â±0.0586 | 0.5663Â±0.1027 | 0.5712Â±0.0675 | 0.5833Â±0.0612 | 0.5741Â±0.0580 | 0.5922Â±0.0808 |
| flare                     | 0.6143Â±0.0239 | 0.6207Â±0.0187 | 0.6050Â±0.0166 | 0.6126Â±0.0280 | 0.6127Â±0.0248 | 0.6235Â±0.0212 | 0.6212Â±0.0202 | 0.6152Â±0.0255 |
| football-player-position  | 0.8476Â±0.0134 | 0.8456Â±0.0152 | 0.8466Â±0.0184 | 0.8483Â±0.0124 | 0.8491Â±0.0128 | 0.8495Â±0.0133 | 0.8479Â±0.0132 | 0.8507Â±0.0113 |
| golub-1999-v2             | 0.8575Â±0.1599 | 0.9233Â±0.0789 | 0.8984Â±0.1400 | 0.9164Â±0.1282 | 0.9065Â±0.0629 | 0.9250Â±0.0675 | 0.9301Â±0.0671 | 0.8371Â±0.1300 |
| hayes-roth                | 0.8212Â±0.0303 | 0.7848Â±0.0417 | 0.8247Â±0.0573 | 0.8224Â±0.0793 | 0.8020Â±0.0528 | 0.8160Â±0.0474 | 0.8177Â±0.0452 | 0.8212Â±0.0303 |
| heart-h                   | 0.3579Â±0.0385 | 0.3402Â±0.0646 | 0.3581Â±0.0673 | 0.3054Â±0.0637 | 0.2958Â±0.0667 | 0.3602Â±0.0564 | 0.3579Â±0.0562 | 0.3459Â±0.0475 |
| heart-switzerland         | 0.2571Â±0.0463 | 0.2760Â±0.0837 | 0.2455Â±0.0646 | 0.2354Â±0.0555 | 0.2420Â±0.0773 | 0.2883Â±0.0886 | 0.2878Â±0.0842 | 0.2742Â±0.0510 |
| helena                    | 0.2037Â±0.0044 | 0.2124Â±0.0047 | 0.2053Â±0.0051 | 0.0197Â±0.0083 | 0.1903Â±0.0052 | 0.2169Â±0.0033 | 0.2152Â±0.0039 | 0.2102Â±0.0032 |
| hepatitis-c               | 0.5256Â±0.0714 | 0.5570Â±0.0762 | 0.5596Â±0.0546 | 0.5880Â±0.0833 | 0.6150Â±0.0925 | 0.5922Â±0.0498 | 0.5922Â±0.0498 | 0.5773Â±0.0623 |
| hypothyroid               | 0.3608Â±0.0188 | 0.3575Â±0.0236 | 0.3622Â±0.0201 | 0.3544Â±0.0225 | 0.3513Â±0.0207 | 0.3599Â±0.0197 | 0.3626Â±0.0230 | 0.3595Â±0.0179 |
| internet-firewall         | 0.8427Â±0.0375 | 0.8234Â±0.0531 | 0.8291Â±0.0413 | 0.8115Â±0.0950 | 0.8310Â±0.0492 | 0.8234Â±0.0531 | 0.8234Â±0.0531 | 0.8431Â±0.0376 |
| ipums-la-98-small         | 0.4097Â±0.0138 | 0.4079Â±0.0171 | 0.4157Â±0.0185 | 0.4071Â±0.0194 | 0.4057Â±0.0207 | 0.4108Â±0.0160 | 0.4105Â±0.0196 | 0.4105Â±0.0142 |
| ipums-la-99-small         | 0.4437Â±0.0088 | 0.4454Â±0.0145 | 0.4264Â±0.0146 | 0.4435Â±0.0145 | 0.4358Â±0.0126 | 0.4405Â±0.0164 | 0.4439Â±0.0148 | 0.4401Â±0.0135 |
| khan-2001                 | 0.9881Â±0.0357 | 0.9699Â±0.0331 | 0.9945Â±0.0165 | 0.9782Â±0.0348 | 0.9740Â±0.0263 | 0.9845Â±0.0239 | 0.9845Â±0.0239 | 0.9875Â±0.0376 |
| kropt                     | 0.8306Â±0.0083 | 0.8622Â±0.0115 | 0.8003Â±0.0122 | 0.5748Â±0.0984 | 0.8466Â±0.0101 | 0.8632Â±0.0062 | 0.8625Â±0.0080 | 0.8351Â±0.0089 |


ðŸ”§ Installation
```
git clone https://github.com/your-username/higec.git
cd higec
pip install -r requirements.txt
```
Dependencies:
numpy
scikit-learn
xgboost
scipy
matplotlib


ðŸ“Š What This Project Does
HiGEC provides:
- Automated Hierarchy Generation from flat-labeled datasets
- Probabilistic and hybrid Hierarchy Exploitation strategies
- Support for any multi-class base classifier
- Benchmark-ready structure using OpenML datasets


ðŸš€ Quick Start

Run the Example:
```
python run_higec_example.py
```
What It Does:
1. Downloads the Glass dataset from OpenML
2. Performs flat classification using XGBoost
3. Automatically constructs a class hierarchy using TSD (Task Similarity Distance)
4. Trains a hierarchical classifier using the LCL+ scheme
5. Compares F1-score of flat vs hierarchical classification


ðŸ§± Core Components

| Component | Description |
| --- | --- |
| HiGen | Constructs hierarchy using representative- or classifier-based distances |
| hier_binary_tree | Hierarchical classifier wrapper for training/prediction |
| utils.py | Data loader, metric scorer, plotting, label checks |


ðŸ§ª Customization

You can change the following parameters in run_higec_example.py:
```
did_ = 41            # Dataset ID (from OpenML)
hc_type = 'lcl+'     # HC strategy: 'lcl+', 'lcpn', 'lcn+f', etc.
diss_type = 'jsd'    # Dissimilarity type: 'tsd', 'jsd', 'ccm', 'cmd'
build_type = 'hdc'   # Hierarchy build type: 'hac' or 'hdc'
eval_metric = 'f1'   # Metric: 'f1', 'accuracy', 'auc'
```
You can also replace XGBClassifier() with any sklearn-compatible classifier.


ðŸ“ˆ Example Output
```
Extended linkage table for levels:
level_id:0, subsets:[[3, 4], [0, 1, 2, 5]], branch_id:[8, 9]
level_id:1, subsets:[[1, 2], [0, 5], [4], [3]], branch_id:[7, 6, 4, 3]

Performance Comparison:
- Flat Classification (f1): 0.6470
- Hierarchical lcl+f (f1): 0.7377
```
Generated Hierarchy:

![generated_hier](https://github.com/user-attachments/assets/fa009a38-bb18-4355-9249-2e9d4264da18)

ðŸ“‚ Project Structure

â”œâ”€â”€ run_higec_example.py     # Main script

â”œâ”€â”€ HiGen.py                 # Hierarchy generation module

â”œâ”€â”€ HiCl.py                  # Hierarchical classifier logic

â”œâ”€â”€ utils.py                 # Data loading and utility functions

â”œâ”€â”€ README.md

â”œâ”€â”€ requirements.txt

